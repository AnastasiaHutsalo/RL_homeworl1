{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### MDP Value Iteration and Policy Iteration\n",
    "import argparse\n",
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "#from lake_envs import *\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "parser = argparse.ArgumentParser(description='A program to run assignment 1 implementations.',\n",
    "\t\t\t\t\t\t\t\tformatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "\n",
    "parser.add_argument(\"--env\", \n",
    "\t\t\t\t\thelp=\"The name of the environment to run your algorithm on.\", \n",
    "\t\t\t\t\tchoices=[\"Deterministic-4x4-FrozenLake-v0\",\"Stochastic-4x4-FrozenLake-v0\"],\n",
    "\t\t\t\t\tdefault=\"Deterministic-4x4-FrozenLake-v0\")\n",
    "\n",
    "\n",
    "def policy_evaluation(P, nS, nA, policy, gamma=0.9, tol=1e-3):\n",
    "\n",
    "  iter = 3000 #set maximum of iteration\n",
    "  value_function = np.zeros(nS)\n",
    "  \n",
    " \n",
    "  i = 1\n",
    "  d = tol+1\n",
    "  while d > tol and i <= iter:\n",
    "    value_function_new = np.zeros(nS)\n",
    "    for s in range(nS):\n",
    "      # for a in range(nA):\n",
    "\t\t\t\t# P[s][a][0] - transition probability\n",
    "\t\t\t\t# P[s][a][1] - next state\n",
    "\t\t\t\t# P[s][a][2] - reward\n",
    "\t\t\t\t# P[s][a][3] - 1 if next state is target 0 otherwise\n",
    "      a = policy[s]\n",
    "      for p, s_, r, t in P[s][a]:\n",
    "        # if t: #if target                   \n",
    "        #   value_function_new[s] += policy[s]*p*r #no next \n",
    "        # else:\n",
    "        if not t:\n",
    "          value_function_new[s] += r+gamma*p*value_function[int(s_)] #plus next state\n",
    "        else:\n",
    "          value_function_new[s] += r\n",
    "    d = max(abs(value_function-value_function_new))\n",
    "    i+=1\n",
    "    value_function = value_function_new\n",
    "  return value_function\n",
    "\n",
    "\n",
    "def policy_improvement(P, nS, nA, value_from_policy, policy, gamma=0.9):\n",
    "  \"\"\"Given the value function from policy improve the policy.\n",
    "  Parameters\n",
    "  ----------\n",
    "  P, nS, nA, gamma:\n",
    "    defined at beginning of file\n",
    "  value_from_policy: np.ndarray\n",
    "    The value calculated from the policy\n",
    "  policy: np.array\n",
    "    The previous policy.\n",
    "  Returns\n",
    "  -------\n",
    "  new_policy: np.ndarray[nS]\n",
    "    An array of integers. Each integer is the optimal action to take\n",
    "    in that state according to the environment dynamics and the\n",
    "    given value function.\n",
    "  \"\"\"\n",
    "\n",
    "  new_policy = np.zeros(nS, dtype='int')\n",
    " \n",
    "  for s in range(nS):\n",
    "    q_a = np.zeros(nA)\n",
    "    for a in range(nA):\n",
    "      for p, s_, r, t in P[s][a]:\n",
    "        if not t:\n",
    "          q_a[a] += r+gamma*p*value_from_policy[int(s_)]\n",
    "        else:\n",
    "          q_a[a]+=r\n",
    "    new_policy[s] = np.argmax(q_a)\n",
    "  return new_policy\n",
    "\n",
    "\n",
    "def policy_iteration(P, nS, nA, gamma=0.9, tol=10e-3):\n",
    "  \"\"\"Runs policy iteration.\n",
    "\n",
    "\tYou should call the policy_evaluation() and policy_improvement() methods to\n",
    "\timplement this method.\n",
    "\n",
    "\tParameters\n",
    "\t----------\n",
    "\tP, nS, nA, gamma:\n",
    "\t\tdefined at beginning of file\n",
    "\ttol: float\n",
    "\t\ttol parameter used in policy_evaluation()\n",
    "\tReturns:\n",
    "\t----------\n",
    "\tvalue_function: np.ndarray[nS]\n",
    "\tpolicy: np.ndarray[nS]\n",
    "  \"\"\"\n",
    "\n",
    "  value_function = np.zeros(nS)\n",
    "  #policy = np.zeros(nS, dtype=int)\n",
    "  i = 0\n",
    "  d = 1\n",
    "  policy = np.random.randint(nA, size=nS)\n",
    "  while i==0 or d > 0 :\n",
    "    V = policy_evaluation(P, nS, nA, policy, gamma, tol)\n",
    "    policy_improve = policy_improvement(P, nS, nA, V, policy, gamma)\n",
    "    i+=1\n",
    "    d = np.linalg.norm(policy - policy_improve, ord=1)\n",
    "    policy = policy_improve \n",
    "  return value_function, policy\n",
    "\n",
    "def value_iteration(P, nS, nA, gamma=0.9, tol=1e-3):\n",
    "  \"\"\"\n",
    "\tLearn value function and policy by using value iteration method for a given\n",
    "\tgamma and environment.\n",
    "\n",
    "\tParameters:\n",
    "\t----------\n",
    "\tP, nS, nA, gamma:\n",
    "\t\tdefined at beginning of file\n",
    "\ttol: float\n",
    "\t\tTerminate value iteration when\n",
    "\t\t\tmax |value_function(s) - prev_value_function(s)| < tol\n",
    "\tReturns:\n",
    "\t----------\n",
    "\tvalue_function: np.ndarray[nS]\n",
    "\tpolicy: np.ndarray[nS]\n",
    "  \"\"\"\n",
    "\n",
    "  value_function = np.zeros(nS)\n",
    "  policy = np.zeros(nS, dtype=int)\n",
    "  d = tol+1\n",
    "  while d>tol:\n",
    "    value_function_new = np.zeros(nS)\n",
    "    for s in range(nS):\n",
    "      op_a = np.zeros(nA)\n",
    "      for a in range(nA): \n",
    "         for p, s_, r, t in P[s][a]:\n",
    "           if not t:\n",
    "             op_a[a] += r+gamma*p*value_function[int(s_)]\n",
    "           else:\n",
    "             op_a[a] += r\n",
    "      value_function_new[s] = np.max(op_a)\n",
    "      policy[s] = np.argmax(op_a)\n",
    "    d=max(abs(value_function_new-value_function))\n",
    "    value_function = value_function_new\n",
    "  return value_function, policy\n",
    "\n",
    "def render_single(env, policy, max_steps=100):\n",
    "  \"\"\"\n",
    "    This function does not need to be modified\n",
    "    Renders policy once on environment. Watch your agent play!\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env: gym.core.Environment\n",
    "      Environment to play on. Must have nS, nA, and P as\n",
    "      attributes.\n",
    "    Policy: np.array of shape [env.nS]\n",
    "      The action to take at a given state\n",
    "  \"\"\"\n",
    "\n",
    "  episode_reward = 0\n",
    "  ob = env.reset()\n",
    "  for t in range(max_steps):\n",
    "    env.render()\n",
    "    time.sleep(0.25)\n",
    "    a = policy[ob]\n",
    "    ob, rew, done, _ = env.step(a)\n",
    "    episode_reward += rew\n",
    "    if done:\n",
    "      break\n",
    "  env.render();\n",
    "  if not done:\n",
    "    print(\"The agent didn't reach a terminal state in {} steps.\".format(max_steps))\n",
    "  else:\n",
    "  \tprint(\"Episode reward: %f\" % episode_reward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
